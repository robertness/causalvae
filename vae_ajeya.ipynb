{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae-ajeya.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajeyamk/causalvae/blob/master/vae_ajeya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rtbzfmeMGvr",
        "colab_type": "code",
        "outputId": "5511d557-1110-448e-ad34-ce6095067c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!pip3 install pyro-ppl\n",
        "!pip3 install torch torchvision\n",
        "!pip3 install pydrive --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (0.3.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.12.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.5.5)\n",
            "Requirement already satisfied: tqdm>=4.31 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.32.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (2.3.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.1.0)\n",
            "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.16.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already up-to-date: pydrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.7.9)\n",
            "Requirement already satisfied, skipping upgrade: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.11.3)\n",
            "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.0.3)\n",
            "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OouaeUpQMEjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision.datasets as dset\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import pyro\n",
        "from pyro.contrib.examples.util import print_and_log\n",
        "import pyro.distributions as dist\n",
        "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
        "from pyro.optim import Adam\n",
        "\n",
        "# Change figure aesthetics\n",
        "%matplotlib inline\n",
        "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
        "\n",
        "USE_CUDA = True\n",
        "\n",
        "pyro.enable_validation(True)\n",
        "pyro.distributions.enable_validation(False)\n",
        "\n",
        "# from custom_mlp import MLP, Expxt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1p7kozIT9YN",
        "colab_type": "code",
        "outputId": "b43bca1d-270d-4b0f-a5db-72480fc19511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux30_pwSUEJX",
        "colab_type": "code",
        "outputId": "84958572-1246-4eab-f56b-118092b21e7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Hack to get all available GPU ram.\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 10.6 GB  | Proc size: 3.5 GB\n",
            "GPU RAM Free: 14310MB | Used: 769MB | Util   5% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPXd84WMYr3W",
        "colab_type": "code",
        "outputId": "62d05def-f7f5-4b45-ee41-b9632f8a7034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GhM07OkM4_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8E4_2q1NTZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://drive.google.com/open?id=1mAtimHIWHJM2UJNxsIylPyI1jUebGIx-\n",
        "custom_mlp_module = drive.CreateFile({'id':'1mAtimHIWHJM2UJNxsIylPyI1jUebGIx-'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-A-TTIFPfip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_mlp = custom_mlp_module.GetContentFile('custom_mlp.py')\n",
        "from custom_mlp import MLP, Exp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vomid1y5MEjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    This class encapsulates the parameters (neural networks) and models & guides needed to train a\n",
        "    semi-supervised variational auto-encoder on the Dsprites image dataset\n",
        "\n",
        "    :param output_size: size of the tensor representing the class label \n",
        "    :param input_size: size of the tensor representing the image (64*64 = 4096 for our Dsprites dataset\n",
        "                       since we flatten the images and scale the pixels to be in [0,1])\n",
        "    :param z_dim: size of the tensor representing the latent random variable z\n",
        "                  (for our Dsprites dataset)\n",
        "    :param hidden_layers: a tuple (or list) of MLP layers to be used in the neural networks\n",
        "                          representing the parameters of the distributions in our model\n",
        "    :param use_cuda: use GPUs for faster training\n",
        "    :param aux_loss_multiplier: the multiplier to use with the auxiliary loss\n",
        "    \"\"\"\n",
        "    def __init__(self, output_size=6, input_size=4096, z_dim=50, hidden_layers=(500,),\n",
        "                 config_enum=None, use_cuda=USE_CUDA, aux_loss_multiplier=None):\n",
        "\n",
        "        super(SVAE, self).__init__()\n",
        "\n",
        "        # initialize the class with all arguments provided to the constructor\n",
        "        self.output_size = output_size\n",
        "        self.input_size = input_size\n",
        "        self.z_dim = z_dim\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.allow_broadcast = config_enum == 'parallel'\n",
        "        self.use_cuda = use_cuda\n",
        "        self.aux_loss_multiplier = aux_loss_multiplier\n",
        "\n",
        "        # define and instantiate the neural networks representing\n",
        "        # the paramters of various distributions in the model\n",
        "        self.setup_networks()\n",
        "\n",
        "    def setup_networks(self):\n",
        "\n",
        "        z_dim = self.z_dim\n",
        "        hidden_sizes = self.hidden_layers\n",
        "\n",
        "        # define the neural networks used later in the model and the guide.\n",
        "        # these networks are MLPs (multi-layered perceptrons or simple feed-forward networks)\n",
        "        # where the provided activation parameter is used on every linear layer except\n",
        "        # for the output layer where we use the provided output_activation parameter\n",
        "        self.encoder_y = MLP([self.input_size] + hidden_sizes + [self.output_size],\n",
        "                             activation=nn.Softplus,\n",
        "                             output_activation=nn.Softmax,\n",
        "                             allow_broadcast=self.allow_broadcast,\n",
        "                             use_cuda=self.use_cuda)\n",
        "\n",
        "        # a split in the final layer's size is used for multiple outputs\n",
        "        # and potentially applying separate activation functions on them\n",
        "        # e.g. in this network the final output is of size [z_dim,z_dim]\n",
        "        # to produce loc and scale, and apply different activations [None,Exp] on them\n",
        "        self.encoder_z = MLP([self.input_size + self.output_size] +\n",
        "                             hidden_sizes + [[z_dim, z_dim]],\n",
        "                             activation=nn.Softplus,\n",
        "                             output_activation=[None, Exp],\n",
        "                             allow_broadcast=self.allow_broadcast,\n",
        "                             use_cuda=self.use_cuda)\n",
        "\n",
        "        self.decoder = MLP([z_dim + self.output_size] +\n",
        "                           hidden_sizes + [self.input_size],\n",
        "                           activation=nn.Softplus,\n",
        "                           output_activation=nn.Sigmoid,\n",
        "                           allow_broadcast=self.allow_broadcast,\n",
        "                           use_cuda=self.use_cuda)\n",
        "\n",
        "        # using GPUs for faster training of the networks\n",
        "        if self.use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "    def model(self, xs, ys=None):\n",
        "        \"\"\"\n",
        "        The model corresponds to the following generative process:\n",
        "        p(z) = normal(0,I)              # dsprites label (latent)\n",
        "        p(y|x) = categorical(I/10.)     # which digit (supervised)\n",
        "        p(x|y,z) = bernoulli(loc(y,z))   # an image\n",
        "        loc is given by a neural network  `decoder`\n",
        "\n",
        "        :param xs: a batch of scaled vectors of pixels from an image\n",
        "        :param ys: (optional) a batch of the class labels i.e.\n",
        "                   the digit corresponding to the image(s)\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        # register this pytorch module and all of its sub-modules with pyro\n",
        "        pyro.module(\"ss_vae\", self)\n",
        "\n",
        "        batch_size = xs.size(0)\n",
        "        options = dict(dtype=xs.dtype, device=xs.device)\n",
        "        with pyro.plate(\"data\"):\n",
        "\n",
        "            # sample the handwriting style from the constant prior distribution\n",
        "            prior_loc = torch.zeros(batch_size, self.z_dim, **options)\n",
        "            prior_scale = torch.ones(batch_size, self.z_dim, **options)\n",
        "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
        "\n",
        "            # if the label y (which digit to write) is supervised, sample from the\n",
        "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
        "            alpha_prior = torch.ones(batch_size, self.output_size, **options) / (1.0 * self.output_size)\n",
        "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
        "\n",
        "            # finally, score the image (x) using the handwriting style (z) and\n",
        "            # the class label y (which digit to write) against the\n",
        "            # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))\n",
        "            # where `decoder` is a neural network\n",
        "            loc = self.decoder.forward([zs, ys])\n",
        "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
        "            # return the loc so we can visualize it later\n",
        "            return loc\n",
        "\n",
        "    def guide(self, xs, ys=None):\n",
        "        \"\"\"\n",
        "        The guide corresponds to the following:\n",
        "        q(y|x) = categorical(alpha(x))              # infer label from an image\n",
        "        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer latent class from an image and the label\n",
        "        loc, scale are given by a neural network `encoder_z`\n",
        "        alpha is given by a neural network `encoder_y`\n",
        "\n",
        "        :param xs: a batch of scaled vectors of pixels from an image\n",
        "        :param ys: (optional) a batch of the class labels i.e.\n",
        "                   the digit corresponding to the image(s)\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
        "        with pyro.plate(\"data\"):\n",
        "\n",
        "            # if the class label (the digit) is not supervised, sample\n",
        "            # (and score) the digit with the variational distribution\n",
        "            # q(y|x) = categorical(alpha(x))\n",
        "            if ys is None: \n",
        "                alpha = self.encoder_y.forward(xs)\n",
        "                ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha))\n",
        "\n",
        "            # sample (and score) the latent handwriting-style with the variational\n",
        "            # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
        "            loc, scale = self.encoder_z.forward([xs, ys])\n",
        "            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
        "\n",
        "    def classifier(self, xs):\n",
        "        \"\"\"\n",
        "        classify an image (or a batch of images)\n",
        "\n",
        "        :param xs: a batch of scaled vectors of pixels from an image\n",
        "        :return: a batch of the corresponding class labels (as one-hots)\n",
        "        \"\"\"\n",
        "        # use the trained model q(y|x) = categorical(alpha(x))\n",
        "        # compute all class probabilities for the image(s)\n",
        "        alpha = self.encoder_y.forward(xs)\n",
        "\n",
        "        # get the index (digit) that corresponds to\n",
        "        # the maximum predicted class probability\n",
        "        res, ind = torch.topk(alpha, 1)\n",
        "\n",
        "        # convert the digit(s) to one-hot tensor(s)\n",
        "        ys = torch.zeros_like(alpha).scatter_(1, ind, 1.0)\n",
        "        return ys\n",
        "\n",
        "    def model_classify(self, xs, ys=None):\n",
        "        \"\"\"\n",
        "        this model is used to add an auxiliary (supervised) loss as described in the\n",
        "        Kingma et al., \"Semi-Supervised Learning with Deep Generative Models\".  It \n",
        "        probably isn't needed here.\n",
        "        \"\"\"\n",
        "        # register all pytorch (sub)modules with pyro\n",
        "        pyro.module(\"ss_vae\", self)\n",
        "\n",
        "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
        "        with pyro.plate(\"data\"):\n",
        "            # this here is the extra term to yield an auxiliary loss that we do gradient descent on\n",
        "            if ys is not None:\n",
        "                alpha = self.encoder_y.forward(xs)\n",
        "                with pyro.poutine.scale(scale=self.aux_loss_multiplier):\n",
        "                    pyro.sample(\"y_aux\", dist.OneHotCategorical(alpha), obs=ys)\n",
        "\n",
        "    def guide_classify(self, xs, ys=None):\n",
        "        \"\"\"\n",
        "        dummy guide function to accompany model_classify in inference\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPdgbk1kMEjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup_data_loaders(train_x, train_y, test_x, test_y, batch_size=128, use_cuda=USE_CUDA):\n",
        "    train_dset = torch.utils.data.TensorDataset(\n",
        "        torch.from_numpy(train_x.astype(np.float32)).reshape(-1, 4096),\n",
        "        torch.from_numpy(train_y.astype(np.float32))\n",
        "    )\n",
        "    test_dset = torch.utils.data.TensorDataset(\n",
        "        torch.from_numpy(test_x.astype(np.float32)).reshape(-1, 4096),\n",
        "        torch.from_numpy(test_y.astype(np.float32))\n",
        "    )    \n",
        "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
        "    loader = {}\n",
        "    loader[\"sup\"] = torch.utils.data.DataLoader(\n",
        "        dataset=train_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset=test_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
        "    )\n",
        "    loader[\"valid\"] = test_loader\n",
        "    loader[\"test\"] = test_loader\n",
        "    return loader\n",
        "\n",
        "def run_supervized_inference_for_epoch(data_loaders, losses):\n",
        "    \"\"\"\n",
        "    runs the inference algorithm for an epoch\n",
        "    returns the values of all losses separately on supervised and unsupervised parts\n",
        "    \"\"\"\n",
        "    num_losses = len(losses)\n",
        "\n",
        "    # compute number of batches for an epoch\n",
        "    batches_per_epoch = len(data_loaders[\"sup\"])\n",
        "\n",
        "    # initialize variables to store loss values\n",
        "    epoch_losses = [0.] * num_losses\n",
        "\n",
        "    # setup the iterators for training data loaders\n",
        "    sup_iter = iter(data_loaders[\"sup\"])\n",
        "\n",
        "    for i in range(batches_per_epoch):\n",
        "\n",
        "        # extract the corresponding batch\n",
        "        (xs, ys) = next(sup_iter)\n",
        "        xs = xs.cuda()\n",
        "        ys = ys.cuda()\n",
        "        # run the inference for each loss with supervised or un-supervised\n",
        "        # data as arguments\n",
        "        for loss_id in range(num_losses):\n",
        "            new_loss = losses[loss_id].step(xs, ys)\n",
        "            epoch_losses[loss_id] += new_loss\n",
        "\n",
        "    # return the values of all losses\n",
        "    return epoch_losses\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad6H4X0HMEjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_zip = np.load(\n",
        "    '/content/gdrive/My Drive/data-science/causal-ml/projects/dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz',\n",
        "    encoding = 'bytes',\n",
        "    allow_pickle=True\n",
        ")\n",
        "imgs = dataset_zip['imgs']\n",
        "latents_values = dataset_zip['latents_values']\n",
        "latents_classes = dataset_zip['latents_classes']\n",
        "metadata = dataset_zip['metadata'][()]\n",
        "\n",
        "latents_sizes = metadata[b'latents_sizes']\n",
        "latents_bases = np.concatenate((latents_sizes[::-1].cumprod()[::-1][1:],\n",
        "                                np.array([1,])))\n",
        "\n",
        "def latent_to_index(latents):\n",
        "      return np.dot(latents, latents_bases).astype(int)\n",
        "\n",
        "\n",
        "def sample_latent(size=1):\n",
        "    samples = np.zeros((size, latents_sizes.size))\n",
        "    for lat_i, lat_size in enumerate(latents_sizes):\n",
        "        samples[:, lat_i] = np.random.randint(lat_size, size=size)\n",
        "\n",
        "    return samples\n",
        "\n",
        "# Sample latents randomly\n",
        "latents_sampled = sample_latent(size=70000)\n",
        "\n",
        "# Select images\n",
        "indices_sampled = latent_to_index(latents_sampled)\n",
        "imgs_sampled = imgs[indices_sampled]\n",
        "\n",
        "data_loaders = setup_data_loaders(\n",
        "    imgs_sampled[1000:], latents_sampled[1000:],\n",
        "    imgs_sampled[:1000], latents_sampled[:1000],\n",
        "    batch_size=256,\n",
        "    use_cuda=USE_CUDA\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvn38eq0MEjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(data_loader, classifier_fn, batch_size):\n",
        "    \"\"\"\n",
        "    compute the accuracy over the supervised training set or the testing set\n",
        "    \"\"\"\n",
        "    predictions, actuals = [], []\n",
        "\n",
        "    # use the appropriate data loader\n",
        "    \n",
        "    for (xs, ys) in data_loader:\n",
        "        xs = xs.cuda()\n",
        "        ys = ys.cuda()\n",
        "        # use classification function to compute all predictions for each batch\n",
        "        predictions.append(classifier_fn(xs))\n",
        "        actuals.append(ys)\n",
        "\n",
        "    # compute the number of accurate predictions\n",
        "    accurate_preds = 0\n",
        "    for pred, act in zip(predictions, actuals):\n",
        "        for i in range(pred.size(0)):\n",
        "            v = torch.sum(pred[i] == act[i])\n",
        "            accurate_preds += (v.item() == 10)\n",
        "\n",
        "    # calculate the accuracy between 0 and 1\n",
        "    accuracy = (accurate_preds * 1.0) / (len(predictions) * batch_size)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def visualize(s_vae, viz, test_loader):\n",
        "    if viz:\n",
        "        plot_conditional_samples_ssvae(s_vae, viz)\n",
        "        mnist_test_tsne_ssvae(ssvae=s_vae, test_loader=test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pz3enLBMEjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sup_vae = SVAE(\n",
        "    output_size=6,\n",
        "    input_size=4096,\n",
        "    z_dim=50,\n",
        "    hidden_layers=[500],\n",
        "    use_cuda=USE_CUDA,\n",
        "    config_enum=\"parallel\",\n",
        "    aux_loss_multiplier=46\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4kr9m3UMEja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam_params = {\"lr\": 0.00042, \"betas\": (0.9, 0.999)}\n",
        "optimizer = Adam(adam_params)\n",
        "# set up the loss(es) for inference. wrapping the guide in config_enumerate builds\n",
        "# the loss as a sum\n",
        "# by enumerating each class label for the sampled discrete categorical distribution\n",
        "# in the model\n",
        "guide = config_enumerate(sup_vae.guide, \"parallel\", expand=True)\n",
        "elbo = Trace_ELBO(max_plate_nesting=1)\n",
        "loss_basic = SVI(sup_vae.model, guide, optimizer, loss=elbo)\n",
        "\n",
        "# build a list of all losses considered\n",
        "losses = [loss_basic]\n",
        "loss_aux = SVI(sup_vae.model_classify, sup_vae.guide_classify, optimizer, loss=elbo)\n",
        "losses.append(loss_aux)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0dRPWlWeG64",
        "colab_type": "code",
        "outputId": "270c8c1e-5d1d-4af3-9072-5b503f0f88f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "sup_vae"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVAE(\n",
              "  (encoder_y): MLP(\n",
              "    (sequential_mlp): Sequential(\n",
              "      (0): ConcatModule()\n",
              "      (1): DataParallel(\n",
              "        (module): Linear(in_features=4096, out_features=500, bias=True)\n",
              "      )\n",
              "      (2): Softplus(beta=1, threshold=20)\n",
              "      (3): Linear(in_features=500, out_features=6, bias=True)\n",
              "      (4): Softmax()\n",
              "    )\n",
              "  )\n",
              "  (encoder_z): MLP(\n",
              "    (sequential_mlp): Sequential(\n",
              "      (0): ConcatModule()\n",
              "      (1): DataParallel(\n",
              "        (module): Linear(in_features=4102, out_features=500, bias=True)\n",
              "      )\n",
              "      (2): Softplus(beta=1, threshold=20)\n",
              "      (3): ListOutModule(\n",
              "        (0): Sequential(\n",
              "          (0): Linear(in_features=500, out_features=50, bias=True)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): Linear(in_features=500, out_features=50, bias=True)\n",
              "          (1): Exp()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): MLP(\n",
              "    (sequential_mlp): Sequential(\n",
              "      (0): ConcatModule()\n",
              "      (1): DataParallel(\n",
              "        (module): Linear(in_features=56, out_features=500, bias=True)\n",
              "      )\n",
              "      (2): Softplus(beta=1, threshold=20)\n",
              "      (3): Linear(in_features=500, out_features=4096, bias=True)\n",
              "      (4): Sigmoid()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfoFJZPVMEjb",
        "colab_type": "code",
        "outputId": "1c457d2c-6478-412c-99f6-36969a8173f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sup_num = 60000\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "\n",
        "logger = open(\"./tmp.log\", \"w\")\n",
        "# Number of supervised number\n",
        "\n",
        "# initializing local variables to maintain the best validation accuracy\n",
        "# seen across epochs over the supervised training set\n",
        "# and the corresponding testing set and the state of the networks\n",
        "best_valid_acc, corresponding_test_acc = 0.0, 0.0\n",
        "\n",
        "# run inference for a certain number of epochs\n",
        "for i in range(0, num_epochs):\n",
        "\n",
        "    # get the losses for an epoch\n",
        "    epoch_losses_sup = run_supervized_inference_for_epoch(\n",
        "        data_loaders,\n",
        "        losses\n",
        "    )\n",
        "\n",
        "    # compute average epoch losses i.e. losses per example\n",
        "    avg_epoch_losses_sup = map(lambda v: v / sup_num, epoch_losses_sup)\n",
        "\n",
        "    # store the loss and validation/testing accuracies in the logfile\n",
        "    str_loss_sup = \" \".join(map(str, avg_epoch_losses_sup))\n",
        "\n",
        "    str_print = \"{} epoch: avg loss {}\".format(i, \"{}\".format(str_loss_sup))\n",
        "\n",
        "    validation_accuracy = get_accuracy(data_loaders[\"valid\"], sup_vae.classifier, batch_size)\n",
        "    str_print += \" validation accuracy {}\".format(validation_accuracy)\n",
        "\n",
        "    # this test accuracy is only for logging, this is not used\n",
        "    # to make any decisions during training\n",
        "    test_accuracy = get_accuracy(data_loaders[\"test\"], sup_vae.classifier, batch_size)\n",
        "    str_print += \" test accuracy {}\".format(test_accuracy)\n",
        "\n",
        "    # update the best validation accuracy and the corresponding\n",
        "    # testing accuracy and the state of the parent module (including the networks)\n",
        "    if best_valid_acc < validation_accuracy:\n",
        "        best_valid_acc = validation_accuracy\n",
        "        corresponding_test_acc = test_accuracy\n",
        "        \n",
        "    visualize(sup_vae, None, data_loaders[\"test\"])\n",
        "\n",
        "    print_and_log(logger, str_print)\n",
        "\n",
        "final_test_accuracy = get_accuracy(data_loaders[\"test\"], sup_vae.classifier, batch_size)\n",
        "print_and_log(logger, \"best validation accuracy {} corresponding testing accuracy {} \"\n",
        "              \"last testing accuracy {}\".format(best_valid_acc, corresponding_test_acc, final_test_accuracy))\n",
        "\n",
        "# close the logger file object if we opened it earlier\n",
        "logger.close()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 epoch: avg loss 73.4194898127238 10.337283599853516 validation accuracy 0.0 test accuracy 0.0\n",
            "1 epoch: avg loss 73.1135249282837 9.959152315266927 validation accuracy 0.0 test accuracy 0.0\n",
            "2 epoch: avg loss 72.90056605784098 9.599737705485026 validation accuracy 0.0 test accuracy 0.0\n",
            "3 epoch: avg loss 72.72961992340088 9.309077065022786 validation accuracy 0.0 test accuracy 0.0\n",
            "4 epoch: avg loss 72.53395691884359 8.995561258951822 validation accuracy 0.0 test accuracy 0.0\n",
            "5 epoch: avg loss 72.29241495920817 8.699117222086588 validation accuracy 0.0 test accuracy 0.0\n",
            "6 epoch: avg loss 72.0966392674764 8.405718975830078 validation accuracy 0.0 test accuracy 0.0\n",
            "7 epoch: avg loss 71.84623018544515 8.126094775390625 validation accuracy 0.0 test accuracy 0.0\n",
            "8 epoch: avg loss 71.72414229482015 7.864214937337239 validation accuracy 0.0 test accuracy 0.0\n",
            "9 epoch: avg loss 71.49476098785401 7.599710852050781 validation accuracy 0.0 test accuracy 0.0\n",
            "10 epoch: avg loss 71.34665864410401 7.383735764567057 validation accuracy 0.0 test accuracy 0.0\n",
            "11 epoch: avg loss 71.24757972157796 7.4970786661783855 validation accuracy 0.0 test accuracy 0.0\n",
            "12 epoch: avg loss 71.0358139175415 7.385145209757487 validation accuracy 0.0 test accuracy 0.0\n",
            "13 epoch: avg loss 70.79814100087484 6.938219022623698 validation accuracy 0.0 test accuracy 0.0\n",
            "14 epoch: avg loss 70.690160206604 6.606387151082357 validation accuracy 0.0 test accuracy 0.0\n",
            "15 epoch: avg loss 70.46839635569255 6.533454721069336 validation accuracy 0.0 test accuracy 0.0\n",
            "16 epoch: avg loss 70.39087748362223 6.275628011067709 validation accuracy 0.0 test accuracy 0.0\n",
            "17 epoch: avg loss 70.16816714019775 6.026749960327148 validation accuracy 0.0 test accuracy 0.0\n",
            "18 epoch: avg loss 70.02727948557536 5.86120785929362 validation accuracy 0.0 test accuracy 0.0\n",
            "19 epoch: avg loss 69.95993371734619 5.694548964436849 validation accuracy 0.0 test accuracy 0.0\n",
            "20 epoch: avg loss 69.76063533681234 5.675880317179362 validation accuracy 0.0 test accuracy 0.0\n",
            "21 epoch: avg loss 69.65645001780192 5.492274950154623 validation accuracy 0.0 test accuracy 0.0\n",
            "22 epoch: avg loss 69.47397186838786 5.404019834391276 validation accuracy 0.0 test accuracy 0.0\n",
            "23 epoch: avg loss 69.3778068456014 5.669653151448568 validation accuracy 0.0 test accuracy 0.0\n",
            "24 epoch: avg loss 69.24922564442953 5.432655401611328 validation accuracy 0.0 test accuracy 0.0\n",
            "25 epoch: avg loss 69.02797335764566 5.363767221069336 validation accuracy 0.0 test accuracy 0.0\n",
            "26 epoch: avg loss 68.97380993804931 5.430132021077474 validation accuracy 0.0 test accuracy 0.0\n",
            "27 epoch: avg loss 68.8614875096639 5.341963358561198 validation accuracy 0.0 test accuracy 0.0\n",
            "28 epoch: avg loss 68.72352076975504 5.49090717569987 validation accuracy 0.0 test accuracy 0.0\n",
            "29 epoch: avg loss 68.57598680165609 5.599548092651367 validation accuracy 0.0 test accuracy 0.0\n",
            "30 epoch: avg loss 68.42424849599202 5.55614609375 validation accuracy 0.0 test accuracy 0.0\n",
            "31 epoch: avg loss 68.29375264638264 5.479485826619466 validation accuracy 0.0 test accuracy 0.0\n",
            "32 epoch: avg loss 68.1904503595988 5.2717307159423825 validation accuracy 0.0 test accuracy 0.0\n",
            "33 epoch: avg loss 68.08769537913004 5.024212093098958 validation accuracy 0.0 test accuracy 0.0\n",
            "34 epoch: avg loss 67.929296534729 4.744880869547526 validation accuracy 0.0 test accuracy 0.0\n",
            "35 epoch: avg loss 67.83656690419515 4.545162293497722 validation accuracy 0.0 test accuracy 0.0\n",
            "36 epoch: avg loss 67.77494875640869 4.303533932495117 validation accuracy 0.0 test accuracy 0.0\n",
            "37 epoch: avg loss 67.67430809885661 4.098325904337565 validation accuracy 0.0 test accuracy 0.0\n",
            "38 epoch: avg loss 67.48732435862223 3.956320953369141 validation accuracy 0.0 test accuracy 0.0\n",
            "39 epoch: avg loss 67.43751215159098 3.842155121358236 validation accuracy 0.0 test accuracy 0.0\n",
            "40 epoch: avg loss 67.3197662612915 3.7160155166625977 validation accuracy 0.0 test accuracy 0.0\n",
            "41 epoch: avg loss 67.20157636871338 3.5679363571166993 validation accuracy 0.0 test accuracy 0.0\n",
            "42 epoch: avg loss 67.11693908843993 3.5223314137776693 validation accuracy 0.0 test accuracy 0.0\n",
            "43 epoch: avg loss 66.99766852366129 3.3062897201538086 validation accuracy 0.0 test accuracy 0.0\n",
            "44 epoch: avg loss 66.94425606435139 3.274656864420573 validation accuracy 0.0 test accuracy 0.0\n",
            "45 epoch: avg loss 66.87721158192953 3.2535403299967447 validation accuracy 0.0 test accuracy 0.0\n",
            "46 epoch: avg loss 66.76631597646077 3.224982407124837 validation accuracy 0.0 test accuracy 0.0\n",
            "47 epoch: avg loss 66.68057815093994 3.1721942459106445 validation accuracy 0.0 test accuracy 0.0\n",
            "48 epoch: avg loss 66.53252608388264 2.9452966857910154 validation accuracy 0.0 test accuracy 0.0\n",
            "49 epoch: avg loss 66.4851595067342 2.8821236943562827 validation accuracy 0.0 test accuracy 0.0\n",
            "50 epoch: avg loss 66.37938383127849 3.09201468556722 validation accuracy 0.0 test accuracy 0.0\n",
            "51 epoch: avg loss 66.31664015452067 3.177724250793457 validation accuracy 0.0 test accuracy 0.0\n",
            "52 epoch: avg loss 66.19034068349202 2.9874297996520998 validation accuracy 0.0 test accuracy 0.0\n",
            "53 epoch: avg loss 66.05708732248942 3.232627372741699 validation accuracy 0.0 test accuracy 0.0\n",
            "54 epoch: avg loss 66.01276608225504 3.173233707173665 validation accuracy 0.0 test accuracy 0.0\n",
            "55 epoch: avg loss 65.88708648427327 3.110121321105957 validation accuracy 0.0 test accuracy 0.0\n",
            "56 epoch: avg loss 65.81027199045818 3.161735363260905 validation accuracy 0.0 test accuracy 0.0\n",
            "57 epoch: avg loss 65.73718862457275 3.3055569646199543 validation accuracy 0.0 test accuracy 0.0\n",
            "58 epoch: avg loss 65.66992208811442 3.2017634826660157 validation accuracy 0.0 test accuracy 0.0\n",
            "59 epoch: avg loss 65.60472373199462 3.028648991394043 validation accuracy 0.0 test accuracy 0.0\n",
            "60 epoch: avg loss 65.51507672678629 3.0213762514750164 validation accuracy 0.0 test accuracy 0.0\n",
            "61 epoch: avg loss 65.38191290842693 3.0188735694885254 validation accuracy 0.0 test accuracy 0.0\n",
            "62 epoch: avg loss 65.30616182607015 2.9055699577331544 validation accuracy 0.0 test accuracy 0.0\n",
            "63 epoch: avg loss 65.21349840647379 2.852216015370687 validation accuracy 0.0 test accuracy 0.0\n",
            "64 epoch: avg loss 65.18253641916911 2.813871074167887 validation accuracy 0.0 test accuracy 0.0\n",
            "65 epoch: avg loss 65.12543276519776 2.7915143440246584 validation accuracy 0.0 test accuracy 0.0\n",
            "66 epoch: avg loss 65.08151169586182 2.802122728474935 validation accuracy 0.0 test accuracy 0.0\n",
            "67 epoch: avg loss 64.97802228342692 2.7456317456563313 validation accuracy 0.0 test accuracy 0.0\n",
            "68 epoch: avg loss 64.95747393544515 2.843347436014811 validation accuracy 0.0 test accuracy 0.0\n",
            "69 epoch: avg loss 64.95950721995035 2.7493250696818032 validation accuracy 0.0 test accuracy 0.0\n",
            "70 epoch: avg loss 64.85206590321859 2.549251330820719 validation accuracy 0.0 test accuracy 0.0\n",
            "71 epoch: avg loss 64.77169641265868 2.617051190185547 validation accuracy 0.0 test accuracy 0.0\n",
            "72 epoch: avg loss 64.71160574696859 2.5421745646158853 validation accuracy 0.0 test accuracy 0.0\n",
            "73 epoch: avg loss 64.58634377593994 2.465168037160238 validation accuracy 0.0 test accuracy 0.0\n",
            "74 epoch: avg loss 64.57885850575765 2.361467325337728 validation accuracy 0.0 test accuracy 0.0\n",
            "75 epoch: avg loss 64.42846132151286 2.4082817609151204 validation accuracy 0.0 test accuracy 0.0\n",
            "76 epoch: avg loss 64.38538060048421 2.298162012990316 validation accuracy 0.0 test accuracy 0.0\n",
            "77 epoch: avg loss 64.3141145767212 2.2881553919474285 validation accuracy 0.0 test accuracy 0.0\n",
            "78 epoch: avg loss 64.24270374501546 2.2224127756754557 validation accuracy 0.0 test accuracy 0.0\n",
            "79 epoch: avg loss 64.09710526682535 2.1379135854085285 validation accuracy 0.0 test accuracy 0.0\n",
            "80 epoch: avg loss 64.07882555491129 2.261145238494873 validation accuracy 0.0 test accuracy 0.0\n",
            "81 epoch: avg loss 64.05341328277588 2.1424091829935707 validation accuracy 0.0 test accuracy 0.0\n",
            "82 epoch: avg loss 63.90139794260661 2.0546640805562335 validation accuracy 0.0 test accuracy 0.0\n",
            "83 epoch: avg loss 63.886747120666506 2.0148360023498535 validation accuracy 0.0 test accuracy 0.0\n",
            "84 epoch: avg loss 63.863132594299316 2.0213047768910726 validation accuracy 0.0 test accuracy 0.0\n",
            "85 epoch: avg loss 63.72724614003499 1.9374477088928224 validation accuracy 0.0 test accuracy 0.0\n",
            "86 epoch: avg loss 63.707502190653486 1.969299099858602 validation accuracy 0.0 test accuracy 0.0\n",
            "87 epoch: avg loss 63.60284013824463 1.9574982669830323 validation accuracy 0.0 test accuracy 0.0\n",
            "88 epoch: avg loss 63.60544111480713 2.1192375803629555 validation accuracy 0.0 test accuracy 0.0\n",
            "89 epoch: avg loss 63.47691573232015 2.071177163950602 validation accuracy 0.0 test accuracy 0.0\n",
            "90 epoch: avg loss 63.45331827952067 1.870301713816325 validation accuracy 0.0 test accuracy 0.0\n",
            "91 epoch: avg loss 63.43649330393473 1.7918146341959635 validation accuracy 0.0 test accuracy 0.0\n",
            "92 epoch: avg loss 63.37312573394775 1.8475859204610188 validation accuracy 0.0 test accuracy 0.0\n",
            "93 epoch: avg loss 63.23665246734619 1.9314768585205078 validation accuracy 0.0 test accuracy 0.0\n",
            "94 epoch: avg loss 63.206928472391766 1.8032865263621012 validation accuracy 0.0 test accuracy 0.0\n",
            "95 epoch: avg loss 63.235852890523276 1.798199406305949 validation accuracy 0.0 test accuracy 0.0\n",
            "96 epoch: avg loss 63.176236850484216 1.788236808013916 validation accuracy 0.0 test accuracy 0.0\n",
            "97 epoch: avg loss 63.0884791112264 1.8939534503936768 validation accuracy 0.0 test accuracy 0.0\n",
            "98 epoch: avg loss 63.16810416819254 1.7823999228159586 validation accuracy 0.0 test accuracy 0.0\n",
            "99 epoch: avg loss 63.05663078358968 1.8215582843780518 validation accuracy 0.0 test accuracy 0.0\n",
            "best validation accuracy 0.0 corresponding testing accuracy 0.0 last testing accuracy 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei0yHxTNMEjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reconstruct_image_w_label(xs, ys, vae):\n",
        "    # backward\n",
        "    xs = xs.cuda()\n",
        "    ys = ys.cuda()\n",
        "    sim_z_loc, sim_z_scale = vae.encoder_z([xs, ys])\n",
        "    zs = dist.Normal(sim_z_loc, sim_z_scale).to_event(1).sample()\n",
        "    # forward\n",
        "    zs = zs.cuda()\n",
        "    ys = ys.cuda()\n",
        "    loc = vae.decoder([zs, ys])\n",
        "    return dist.Bernoulli(loc).to_event(1).sample()\n",
        "\n",
        "def reconstruct_image(xs, vae):\n",
        "    # backward\n",
        "    xs = xs.cuda()\n",
        "    sim_ys = vae.encoder_y(xs)\n",
        "    sim_z_loc, sim_z_scale = vae.encoder_z([xs, sim_ys])\n",
        "    zs = dist.Normal(sim_z_loc, sim_z_scale).to_event(1).sample()\n",
        "    # forward\n",
        "    zs = zs.cuda()\n",
        "    loc = vae.decoder([zs, ys.cuda()])\n",
        "    return dist.Bernoulli(loc).to_event(1).sample()\n",
        "\n",
        "def convert_back(x):\n",
        "    return x.cpu().reshape(-1, 64, 64).numpy().astype(np.uint8)\n",
        "\n",
        "def show_images_grid(imgs_, num_images):\n",
        "    ncols = int(np.ceil(num_images**0.5))\n",
        "    nrows = int(np.ceil(num_images / ncols))\n",
        "    _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
        "    axes = axes.flatten()\n",
        "    for ax_i, ax in enumerate(axes):\n",
        "        if ax_i < num_images:\n",
        "          ax.imshow(imgs_[ax_i].reshape(64, 64), cmap='Greys_r',  interpolation='nearest')\n",
        "          ax.set_xticks([])\n",
        "          ax.set_yticks([])\n",
        "        else:\n",
        "          ax.axis('off')\n",
        "\n",
        "def viz_images(imgs, n):\n",
        "    imgs_ = []\n",
        "    for i, x in enumerate(imgs):\n",
        "        if i > n:\n",
        "            break\n",
        "        imgs_.append(convert_back(x))\n",
        "    show_images_grid(np.array(imgs_), n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm-CLHjBMEjg",
        "colab_type": "text"
      },
      "source": [
        "Four original images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7trYLCqMEjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sup_iter = iter(data_loaders[\"sup\"])\n",
        "xs, ys = next(sup_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVQmHBd_MEji",
        "colab_type": "code",
        "outputId": "d480f485-19bc-4c12-c317-735bdafa8eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "print(\"With y and x:\")\n",
        "xs_sim1 = reconstruct_image_w_label(xs, ys, sup_vae)\n",
        "viz_images(xs_sim1, 4)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With y and x:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAFmCAYAAACr2LumAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x432 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEvI9XGTMEjk",
        "colab_type": "code",
        "outputId": "b7db84ee-5957-4f0c-c35f-e9e7f42c2ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "print(\"With just x:\")\n",
        "# backward\n",
        "xs_sim2 = reconstruct_image(xs, sup_vae)\n",
        "viz_images(xs_sim2, 4)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With just x:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAFmCAYAAACr2LumAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP3rLqigPu5+WAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x432 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_OtIYKxdNp-",
        "colab_type": "code",
        "outputId": "edf1ceff-99c8-421c-86d9-b4a3766e64b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "xs_sim2.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 4096])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCZoEdLh0rET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}